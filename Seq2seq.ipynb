{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gugupig/Seq2seq/blob/main/Seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK2oBAYuiZPX"
      },
      "source": [
        "Downloading the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWGa9FXifHzO"
      },
      "source": [
        "# Imports Pytorch.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VXDpa1tiSfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc0a310-c06c-4dc2-f4d5-eaefa5cfce4e"
      },
      "source": [
        "#Downloads the dataset.\n",
        "#use 2 books to train\n",
        "import urllib\n",
        "books  =  [urllib.request.urlretrieve(\"https://www.gutenberg.org/cache/epub/14155/pg14155.txt\")[0],urllib.request.urlretrieve(\"https://www.gutenberg.org/cache/epub/17489/pg17489.txt\")[0]]\n",
        "for d in books:\n",
        "  print(d)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmpq6j7vrcn\n",
            "/tmp/tmpdnkzs05v\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU4hirpsiWX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862515a3-28e4-4dbc-86ad-6a8f4bc11ce3"
      },
      "source": [
        "# Prints the first 200 lines in the file with their line number.\n",
        "# This shows that we have a little bit of preprocessing to do in order to clean the data.\n",
        "filename  = books[0]\n",
        "with open(filename,encoding=\"utf-8\") as f:\n",
        "  for i in range(200):\n",
        "    print(f\"[{i}] {f.readline()}\", end='')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] ﻿The Project Gutenberg EBook of Madame Bovary, by Gustave Flaubert\n",
            "[1] \n",
            "[2] This eBook is for the use of anyone anywhere at no cost and with\n",
            "[3] almost no restrictions whatsoever.  You may copy it, give it away or\n",
            "[4] re-use it under the terms of the Project Gutenberg License included\n",
            "[5] with this eBook or online at www.gutenberg.org\n",
            "[6] \n",
            "[7] \n",
            "[8] Title: Madame Bovary\n",
            "[9] \n",
            "[10] Author: Gustave Flaubert\n",
            "[11] \n",
            "[12] Release Date: November 26, 2004 [EBook #14155]\n",
            "[13] [Last updated: November 28, 2011]\n",
            "[14] \n",
            "[15] \n",
            "[16] Language: French\n",
            "[17] \n",
            "[18] \n",
            "[19] *** START OF THIS PROJECT GUTENBERG EBOOK MADAME BOVARY ***\n",
            "[20] \n",
            "[21] \n",
            "[22] \n",
            "[23] \n",
            "[24] Produced by Ebooks libres et gratuits at http://www.ebooksgratuits.com\n",
            "[25] \n",
            "[26] \n",
            "[27] \n",
            "[28] \n",
            "[29] \n",
            "[30] Gustave Flaubert\n",
            "[31] MADAME BOVARY\n",
            "[32] \n",
            "[33] \n",
            "[34] (1857)\n",
            "[35] \n",
            "[36] \n",
            "[37] Table des matières\n",
            "[38] \n",
            "[39] PREMIÈRE PARTIE\n",
            "[40] I\n",
            "[41] II\n",
            "[42] III\n",
            "[43] IV\n",
            "[44] V\n",
            "[45] VI\n",
            "[46] VII\n",
            "[47] VIII\n",
            "[48] IX\n",
            "[49] DEUXIÈME PARTIE\n",
            "[50] I\n",
            "[51] II\n",
            "[52] III\n",
            "[53] IV\n",
            "[54] V\n",
            "[55] VI\n",
            "[56] VII\n",
            "[57] VIII\n",
            "[58] IX\n",
            "[59] X\n",
            "[60] XI\n",
            "[61] XII\n",
            "[62] XIII\n",
            "[63] XIV\n",
            "[64] XV\n",
            "[65] TROISIÈME PARTIE\n",
            "[66] I\n",
            "[67] II\n",
            "[68] III\n",
            "[69] IV\n",
            "[70] V\n",
            "[71] VI\n",
            "[72] VII\n",
            "[73] VIII\n",
            "[74] IX\n",
            "[75] X\n",
            "[76] XI\n",
            "[77] \n",
            "[78] \n",
            "[79] À Marie-Antoine-Jules Senard\n",
            "[80] \n",
            "[81] MEMBRE DU BARREAU DE PARIS EX-PRESIDENT DE L'ASSEMBLÉE NATIONALE\n",
            "[82] ET ANCIEN MINISTRE DE L'INTÉRIEUR\n",
            "[83] \n",
            "[84] Cher et illustre ami,\n",
            "[85] \n",
            "[86] Permettez-moi d'inscrire votre nom en tête de ce livre et au-\n",
            "[87] dessus même de sa dédicace; car c'est à vous, surtout, que j'en\n",
            "[88] dois la publication. En passant par votre magnifique plaidoirie,\n",
            "[89] mon oeuvre a acquis pour moi-même comme une autorité imprévue.\n",
            "[90] Acceptez donc ici l'hommage de ma gratitude, qui, si grande\n",
            "[91] qu'elle puisse être, ne sera jamais à la hauteur de votre\n",
            "[92] éloquence et de votre dévouement.\n",
            "[93] \n",
            "[94] GUSTAVE FLAUBERT\n",
            "[95] \n",
            "[96] Paris, 12 avril 1857\n",
            "[97] \n",
            "[98] \n",
            "[99] À Louis Bouilhet\n",
            "[100] \n",
            "[101] \n",
            "[102] PREMIÈRE PARTIE\n",
            "[103] \n",
            "[104] \n",
            "[105] I\n",
            "[106] \n",
            "[107] Nous étions à l'Étude, quand le Proviseur entra, suivi d'un\n",
            "[108] nouveau habillé en bourgeois et d'un garçon de classe qui portait\n",
            "[109] un grand pupitre. Ceux qui dormaient se réveillèrent, et chacun se\n",
            "[110] leva comme surpris dans son travail.\n",
            "[111] \n",
            "[112] Le Proviseur nous fit signe de nous rasseoir; puis, se tournant\n",
            "[113] vers le maître d'études:\n",
            "[114] \n",
            "[115] -- Monsieur Roger, lui dit-il à demi-voix, voici un élève que je\n",
            "[116] vous recommande, il entre en cinquième. Si son travail et sa\n",
            "[117] conduite sont méritoires, il passera dans les grands, où l'appelle\n",
            "[118] son âge.\n",
            "[119] \n",
            "[120] Resté dans l'angle, derrière la porte, si bien qu'on l'apercevait\n",
            "[121] à peine, le nouveau était un gars de la campagne, d'une quinzaine\n",
            "[122] d'années environ, et plus haut de taille qu'aucun de nous tous. Il\n",
            "[123] avait les cheveux coupés droit sur le front, comme un chantre de\n",
            "[124] village, l'air raisonnable et fort embarrassé. Quoiqu'il ne fût\n",
            "[125] pas large des épaules, son habit-veste de drap vert à boutons\n",
            "[126] noirs devait le gêner aux entournures et laissait voir, par la\n",
            "[127] fente des parements, des poignets rouges habitués à être nus. Ses\n",
            "[128] jambes, en bas bleus, sortaient d'un pantalon jaunâtre très tiré\n",
            "[129] par les bretelles. Il était chaussé de souliers forts, mal cirés,\n",
            "[130] garnis de clous.\n",
            "[131] \n",
            "[132] On commença la récitation des leçons. Il les écouta de toutes ses\n",
            "[133] oreilles, attentif comme au sermon, n'osant même croiser les\n",
            "[134] cuisses, ni s'appuyer sur le coude, et, à deux heures, quand la\n",
            "[135] cloche sonna, le maître d'études fut obligé de l'avertir, pour\n",
            "[136] qu'il se mît avec nous dans les rangs.\n",
            "[137] \n",
            "[138] Nous avions l'habitude, en entrant en classe, de jeter nos\n",
            "[139] casquettes par terre, afin d'avoir ensuite nos mains plus libres;\n",
            "[140] il fallait, dès le seuil de la porte, les lancer sous le banc, de\n",
            "[141] façon à frapper contre la muraille en faisant beaucoup de\n",
            "[142] poussière; c'était là le genre.\n",
            "[143] \n",
            "[144] Mais, soit qu'il n'eût pas remarqué cette manoeuvre ou qu'il n'eut\n",
            "[145] osé s'y soumettre, la prière était finie que le nouveau tenait\n",
            "[146] encore sa casquette sur ses deux genoux. C'était une de ces\n",
            "[147] coiffures d'ordre composite, où l'on retrouve les éléments du\n",
            "[148] bonnet à poil, du chapska, du chapeau rond, de la casquette de\n",
            "[149] loutre et du bonnet de coton, une de ces pauvres choses, enfin,\n",
            "[150] dont la laideur muette a des profondeurs d'expression comme le\n",
            "[151] visage d'un imbécile. Ovoïde et renflée de baleines, elle\n",
            "[152] commençait par trois boudins circulaires; puis s'alternaient,\n",
            "[153] séparés par une bande rouge, des losanges de velours et de poils\n",
            "[154] de lapin; venait ensuite une façon de sac qui se terminait par un\n",
            "[155] polygone cartonné, couvert d'une broderie en soutache compliquée,\n",
            "[156] et d'où pendait, au bout d'un long cordon trop mince, un petit\n",
            "[157] croisillon de fils d'or, en manière de gland. Elle était neuve; la\n",
            "[158] visière brillait.\n",
            "[159] \n",
            "[160] -- Levez-vous, dit le professeur.\n",
            "[161] \n",
            "[162] Il se leva; sa casquette tomba. Toute la classe se mit à rire.\n",
            "[163] \n",
            "[164] Il se baissa pour la reprendre. Un voisin la fit tomber d'un coup\n",
            "[165] de coude, il la ramassa encore une fois.\n",
            "[166] \n",
            "[167] -- Débarrassez-vous donc de votre casque, dit le professeur, qui\n",
            "[168] était un homme d'esprit.\n",
            "[169] \n",
            "[170] Il y eut un rire éclatant des écoliers qui décontenança le pauvre\n",
            "[171] garçon, si bien qu'il ne savait s'il fallait garder sa casquette à\n",
            "[172] la main, la laisser par terre ou la mettre sur sa tête. Il se\n",
            "[173] rassit et la posa sur ses genoux.\n",
            "[174] \n",
            "[175] -- Levez-vous, reprit le professeur, et dites-moi votre nom.\n",
            "[176] \n",
            "[177] Le nouveau articula, d'une voix bredouillante, un nom\n",
            "[178] inintelligible.\n",
            "[179] \n",
            "[180] -- Répétez!\n",
            "[181] \n",
            "[182] Le même bredouillement de syllabes se fit entendre, couvert par\n",
            "[183] les huées de la classe.\n",
            "[184] \n",
            "[185] -- Plus haut! cria le maître, plus haut!\n",
            "[186] \n",
            "[187] Le nouveau, prenant alors une résolution extrême, ouvrit une\n",
            "[188] bouche démesurée et lança à pleins poumons, comme pour appeler\n",
            "[189] quelqu'un, ce mot: Charbovari.\n",
            "[190] \n",
            "[191] Ce fut un vacarme qui s'élança d'un bond, monta en crescendo, avec\n",
            "[192] des éclats de voix aigus (on hurlait, on aboyait, on trépignait,\n",
            "[193] on répétait: Charbovari! Charbovari!), puis qui roula en notes\n",
            "[194] isolées, se calmant à grand-peine, et parfois qui reprenait tout à\n",
            "[195] coup sur la ligne d'un banc où saillissait encore çà et là, comme\n",
            "[196] un pétard mal éteint, quelque rire étouffé.\n",
            "[197] \n",
            "[198] Cependant, sous la pluie des pensums, l'ordre peu à peu se\n",
            "[199] rétablit dans la classe, et le professeur, parvenu à saisir le nom\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctl4Z9Gti-6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c311e0bb-b934-4f87-a443-26fa0a34890b"
      },
      "source": [
        "skiping_para = [\"PREMIÈRE PARTIE\",\"Livre premier--Un juste\"]\n",
        "end_para = [\"End of the Project Gutenberg EBook of Madame Bovary, by Gustave Flaubert\",\"End of the Project Gutenberg EBook of Les misérables Tome I, by Victor Hugo\"]\n",
        "import re # Regular expression library\n",
        "roman_regex = re.compile('^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$') # This regular expression matches Roman numerals but also the empty string.\n",
        "EOP = '\\n' # The end-of-line character will be used to mark the end of paragraphs.\n",
        "book_1  =  books[0]\n",
        "book_2  = books[1]\n",
        "\n",
        "def make_paragraphs(filename):\n",
        "  \n",
        "  with open(filename,encoding=\"utf-8\") as f:\n",
        "    # skip everything before the actual text of the novel.\n",
        "    # The line \"PREMIÈRE PARTIE\" appears twice: in the table of content and then at the start of the first part of the actual text.\n",
        "    # The following lines discard everything up to this second occurence (included).\n",
        "    skip = 2\n",
        "    while(skip > 0):\n",
        "      line = f.readline().strip()\n",
        "      if(line in skiping_para): skip -= 1;\n",
        "\n",
        "    paragraphs = [] # each dialog line will be considered a separate paragraph.\n",
        "    paragraph_buffer = [] # List[str]; each element corresponds to a line in the original text file + an additonal space if necessary.\n",
        "    while(True):\n",
        "      line = f.readline().strip()\n",
        "      if(line in end_para): break # End of the actual text.\n",
        "\n",
        "      if(line == \"\"): #the end of a paragraph.\n",
        "        if(len(paragraph_buffer) > 0):\n",
        "          paragraph_buffer.append(EOP) # End of the paragraph.\n",
        "\n",
        "          paragraph = \"\".join(paragraph_buffer) # The different lines that make up the paragraph are joined into a single string.\n",
        "          paragraphs.append(paragraph)\n",
        "          paragraph_buffer = []\n",
        "        continue\n",
        "\n",
        "      if(roman_regex.match(line)): continue # Ignores the lines that indicate the beginning of a chapter.\n",
        "      if(line.endswith(\" PARTIE\")): continue # Ignores the lines that indicate the beginning of a part.\n",
        "\n",
        "      if((len(paragraph_buffer) > 0) and (paragraph_buffer[-1][-1] != '-')): paragraph_buffer.append(' ') # Adds a space between consecutive lines except when the first one ends with \"-\" (e.g. if the word \"pomme-de-terre\" is split with \"pomme-de-\" at the end of a line and \"terre\" at the beginning of the next, we do not want to join the two lines with a space).\n",
        "      paragraph_buffer.append(line)\n",
        "    return paragraphs\n",
        "\n",
        "\n",
        "paragraphs  = make_paragraphs(book_1)\n",
        "paragraphs_2  = make_paragraphs(book_2)\n",
        "print(f\"{len(paragraphs)} paragraphs read.\")\n",
        "for i in range(3): print(paragraphs[i], end='')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2995 paragraphs read.\n",
            "Nous étions à l'Étude, quand le Proviseur entra, suivi d'un nouveau habillé en bourgeois et d'un garçon de classe qui portait un grand pupitre. Ceux qui dormaient se réveillèrent, et chacun se leva comme surpris dans son travail.\n",
            "Le Proviseur nous fit signe de nous rasseoir; puis, se tournant vers le maître d'études:\n",
            "-- Monsieur Roger, lui dit-il à demi-voix, voici un élève que je vous recommande, il entre en cinquième. Si son travail et sa conduite sont méritoires, il passera dans les grands, où l'appelle son âge.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAn1SqQE1Hr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "792a9367-35c7-4ac4-aa77-d7347310863d"
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "# Computes the frequency of all characters in the dataset.\n",
        "def  count_char(g):\n",
        "    char_counts = collections.defaultdict(int)\n",
        "    for paragraph in g:\n",
        "      for char in paragraph: char_counts[char] += 1\n",
        "    return char_counts\n",
        "\n",
        "paragraphs_all = paragraphs.copy()\n",
        "paragraphs_all.extend(paragraphs_2)\n",
        "char_counts = count_char (paragraphs_all)\n",
        "print(f\"{len(char_counts)} different characters found in the dataset.\")\n",
        "print(sorted(char_counts.items(), key=(lambda x: x[1]), reverse=True)) # Shows each character with its frequency, in decreasing frequency order.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102 different characters found in the dataset.\n",
            "[(' ', 216077), ('e', 151088), ('a', 86232), ('s', 80014), ('t', 77066), ('i', 75967), ('n', 69471), ('r', 65834), ('u', 64636), ('l', 62639), ('o', 52942), ('d', 36323), ('c', 28988), ('m', 28812), ('p', 26509), (',', 21198), ('é', 18018), ('v', 17966), (\"'\", 15056), ('.', 12908), ('q', 11734), ('f', 10856), ('h', 10329), ('b', 10209), ('g', 9056), ('-', 8446), ('\\n', 5873), ('à', 5392), ('x', 4063), ('j', 3775), ('y', 3326), ('è', 3324), ('ê', 2758), ('!', 2375), (';', 2267), ('C', 2156), ('L', 2145), ('I', 2108), ('E', 2096), ('M', 1925), ('z', 1615), ('?', 1195), (':', 1097), ('J', 1057), ('A', 1007), ('P', 894), ('D', 846), ('â', 816), ('ç', 813), ('V', 751), ('S', 742), ('B', 680), ('O', 637), ('T', 620), ('î', 602), ('ù', 576), ('û', 576), ('ô', 567), ('Q', 559), ('_', 556), ('F', 465), ('R', 401), ('N', 340), ('«', 335), ('H', 330), ('U', 323), ('»', 262), ('À', 203), ('G', 185), ('1', 124), ('Y', 89), ('É', 84), ('ï', 83), ('(', 69), (')', 69), ('8', 56), ('X', 47), ('9', 32), ('2', 30), ('k', 29), ('7', 26), ('3', 26), ('5', 25), ('\"', 22), ('Ç', 19), ('Ô', 19), ('6', 18), ('Z', 18), ('4', 15), ('ë', 14), ('Ê', 13), ('0', 13), ('W', 9), ('*', 8), ('ü', 7), ('°', 7), ('w', 6), ('Â', 2), ('ñ', 2), ('È', 2), ('Î', 1), ('/', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF0YFZTxD_A0"
      },
      "source": [
        "# build a dictionary 'char_vocabulary' that assigns an integer id to each character, along with list/array 'id_to_char' that implement the reverse mapping.\n",
        "#################\n",
        "def make_maps(char_counts):\n",
        "    id_to_char  = []\n",
        "    char_vocabulary =  {}\n",
        "    for char in char_counts.keys():\n",
        "      id_to_char.append(char)\n",
        "    id_to_char.remove(EOP) # [Timothée Bernard] ?? à expliquer\n",
        "    id_to_char.append(EOP) # [Timothée Bernard] ?? à expliquer\n",
        "    id_to_char.append('<p>') # [Timothée Bernard] ?? à expliquer\n",
        "    for char in id_to_char:\n",
        "      char_vocabulary[char] = id_to_char.index(char)\n",
        "    return id_to_char,char_vocabulary\n",
        "id_to_char,char_vocabulary = make_maps(char_counts)\n",
        "#################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C71VlK5e3Gg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad674506-b33f-4968-e332-c9bb1a0f03f3"
      },
      "source": [
        "EOP_id = char_vocabulary[EOP] # Id for the end-of-paragraph symbol\n",
        "PAD_id = char_vocabulary['<p>']\n",
        "\n",
        "print(f\"EOP_id = {EOP_id}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EOP_id = 101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bc3Gst0zhQv"
      },
      "source": [
        "# Turns a list of lists of ids into a list of strings.\n",
        "def ids_to_texts(ids):\n",
        "  # Turn each list of character ids of 'ids' into a string and then return all strings as a list.\n",
        "  #################\n",
        "  return [\"\".join([''.join([id_to_char[id]]) for id in l]) for l in ids]\n",
        "  #################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybAhzb4_3RTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca96901b-6e34-4cc7-9e00-4676cf3bad3e"
      },
      "source": [
        "ps = [\"Bonjour.\", \"Comment allez vous ?\"]\n",
        "ids = [[char_vocabulary[c] for c in p] for p in ps]\n",
        "print(ids)\n",
        "print(ids_to_texts(ids))\n",
        "print(f\"'ids_to_texts(ids) == ps' should be True: {ids_to_texts(ids) == ps}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[60, 1, 8, 40, 1, 2, 19, 27], [28, 1, 30, 30, 14, 8, 6, 4, 17, 10, 10, 14, 44, 4, 20, 1, 2, 3, 4, 61]]\n",
            "['Bonjour.', 'Comment allez vous ?']\n",
            "'ids_to_texts(ids) == ps' should be True: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnT7T-yHEPCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "474796e5-d6ba-472c-f4a1-f440c15ea361"
      },
      "source": [
        "ps = [\"Bonjour.\", \"Comment allez vous ?\"]\n",
        "ids = [[char_vocabulary[c] for c in p] for p in ps]\n",
        "ids[0].extend([EOP_id, (EOP_id+1), (EOP_id+1)]) # With the end-of-paragraph token id and some padding ids for the first string.\n",
        "print(ids)\n",
        "print(ids_to_texts(ids))\n",
        "print(f\"'ids_to_texts(ids) == ps' should be True: {ids_to_texts(ids) == ps}\") # [Timothée Bernard] Le test n'est pas réussi mais vous vous en moquez ?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[60, 1, 8, 40, 1, 2, 19, 27, 101, 102, 102], [28, 1, 30, 30, 14, 8, 6, 4, 17, 10, 10, 14, 44, 4, 20, 1, 2, 3, 4, 61]]\n",
            "['Bonjour.\\n<p><p>', 'Comment allez vous ?']\n",
            "'ids_to_texts(ids) == ps' should be True: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBuCiGqO08nd"
      },
      "source": [
        "Batch generator\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoH4g-Fkkrgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "251b4bc3-03d7-4445-abc3-97c996d0eb90"
      },
      "source": [
        "# Defines a class of objects that produce batches from the dataset.\n",
        "# A training instance is composed of a pair of consecutive paragraphs. The goal will be to predict the second given the first.\n",
        "# (Possible improvement: As is, ends of chapter are completely ignored: the last paragraph of a chapter and the first of the following chapter form a training instance. We might want to predict the end of the chapter instead, or simply remove these pairs from the dataset.)\n",
        "class BatchGenerator:\n",
        "  def __init__(self, paragraphs, char_vocabulary):\n",
        "    self.paragraphs = paragraphs\n",
        "    self.char_vocabulary = char_vocabulary # Dictionary\n",
        "    self.padding_idx = len(char_vocabulary) - 1\n",
        "  \n",
        "  # Returns the number of training instances (i.e. of pairs of consecutive paragraphs).\n",
        "  def length(self):\n",
        "    return (len(self.paragraphs) - 1)\n",
        "\n",
        "  # Returns a random training batch (composed of pairs of consecutive paragraphs).\n",
        "  # If `subset` is an integer, only a subset of the corpus is used. This can be useful to debug the system.\n",
        "  def get_batch(self, batch_size, subset=None):\n",
        "    max_i = self.length() if(subset is None) else min(subset, self.length())\n",
        "    paragraph_ids = np.random.randint(max_i, size=batch_size) # Randomly picks some paragraph ids.\n",
        "\n",
        "    return self._ids_to_batch(paragraph_ids)\n",
        "\n",
        "  def _ids_to_batch(self, paragraph_ids):\n",
        "    firsts = [] # First paragraph of each pair\n",
        "    seconds = [] # Second paragraph of each pair\n",
        "    for paragraph_id in paragraph_ids:\n",
        "      firsts.append([self.char_vocabulary[char] for char in self.paragraphs[paragraph_id]])\n",
        "      seconds.append([self.char_vocabulary[char] for char in self.paragraphs[paragraph_id + 1]])\n",
        "      \n",
        "    # Padding\n",
        "    self.pad(firsts)\n",
        "    self.pad(seconds)\n",
        "\n",
        "    firsts = torch.tensor(firsts, dtype=torch.long) # Conversion to a tensor\n",
        "    seconds = torch.tensor(seconds, dtype=torch.long) # Conversion to a tensor\n",
        "\n",
        "    return (firsts, seconds)\n",
        "  \n",
        "  # Pads a list of lists (i.e. adds fake word ids so that all sequences in the batch have the same length, so that we can use a matrix to represent them).\n",
        "  # In place\n",
        "  def pad(self, sequences):\n",
        "    max_length = max([len(s) for s in sequences])\n",
        "    for s in sequences: s.extend([self.padding_idx] * (max_length - len(s)))\n",
        "  \n",
        "  # Returns a generator of training batches for a full epoch.\n",
        "  # If `subset` is an integer, only a subset of the corpus is used. This can be useful to debug the system.\n",
        "  def all_batches(self, batch_size, subset=None):\n",
        "    max_i = self.length() if(subset is None) else min(subset, self.length())\n",
        "\n",
        "    # Loop that generates all full batches (batches of size 'batch_size')\n",
        "    i = 0\n",
        "    while((i + batch_size) <= max_i):\n",
        "      instance_ids = np.arange(i, (i + batch_size))\n",
        "      yield self._ids_to_batch(instance_ids)\n",
        "      i += batch_size\n",
        "    \n",
        "    # Possibly generates the last (not full) batch.\n",
        "    if(i < max_i):\n",
        "      instance_ids = np.arange(i, max_i)\n",
        "      yield self._ids_to_batch(instance_ids)\n",
        "  \n",
        "  # Turns a list of arbitrary paragraphs into a prediction batch.\n",
        "  def turn_into_batch(self, paragraphs):\n",
        "    firsts = []\n",
        "    for paragraph in paragraphs:\n",
        "        # Unknown characters are ignored (removed).\n",
        "        tmp = []\n",
        "        for char in paragraph:\n",
        "          if(char in self.char_vocabulary): tmp.append(self.char_vocabulary[char])\n",
        "\n",
        "        if(tmp[-1] != EOP_id): tmp.append(EOP_id) # Adds an end-of-paragraph character if necessary.\n",
        "\n",
        "        firsts.append(tmp)\n",
        "    \n",
        "    self.pad(firsts)\n",
        "    return torch.tensor(firsts, dtype=torch.long)\n",
        "\n",
        "batch_generator = BatchGenerator(paragraphs=paragraphs, char_vocabulary=char_vocabulary)\n",
        "print(batch_generator.length())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YSugGI7z3JX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62fbcb63-49eb-4653-984f-b57a9707ee29"
      },
      "source": [
        "(firsts, seconds) = batch_generator.get_batch(3)\n",
        "print(firsts.size())\n",
        "print(seconds.size())\n",
        "print(ids_to_texts(firsts))\n",
        "print(ids_to_texts(seconds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 252])\n",
            "torch.Size([3, 214])\n",
            "['-- Mais enfin, monsieur, fit Emma, vous aviez à me dire...?\\n<p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p>', '-- Avec un sac de nuit.\\n<p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p>', \"-- Moi, je trouve, dit M. Lheureux (s'adressant au pharmacien, qui passait pour gagner sa place), que l'on aurait dû planter là deux mâts vénitiens: avec quelque chose d'un peu sévère et de riche comme nouveautés, c'eût été d'un fort joli coup d'oeil.\\n\"]\n",
            "[\"-- C'est vrai, madame... Votre beau-père est mort!\\n<p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p>\", '-- Décidément, pensa Lheureux, il y a du grabuge là-dessous.\\n<p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p><p>', \"-- Certes, répondit Homais. Mais, que voulez-vous! c'est le maire qui a tout pris sous son bonnet. Il n'a pas grand goût, ce pauvre Tuvache, et il est même complètement dénué de ce qui s'appelle le génie des arts.\\n\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv1Z1s-cQWiT"
      },
      "source": [
        "The model\n",
        "==\n",
        "(Don't forget to read carefully all comments and to make sure you understand them.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKfRCXQOOm8X"
      },
      "source": [
        "class Model(torch.nn.Module):\n",
        "  # 'size_vocabulary' does not include a padding character, but does include the end-of-paragraph one.\n",
        "  def __init__(self, vocabulary_size, EOP_id, embedding_size, lstm_hidden_size, lstm_layers, encoder_dropout_p = 0,decoder_dropout_p = 0 ,device='cpu'):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocabulary_size = vocabulary_size\n",
        "    self.EOP_id = EOP_id\n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_size = lstm_hidden_size\n",
        "    self.lstm_layers = lstm_layers\n",
        "    self.encoder_dropout_p = encoder_dropout_p\n",
        "    self.decoder_dropout_p =  decoder_dropout_p\n",
        "    self.device = device\n",
        "\n",
        "\n",
        "\n",
        "    # At prediction time, this index is used to stop the generation at the end of the paragraph.\n",
        "    self.char_embeddings = nn.Embedding(self.vocabulary_size,\n",
        "                                        self.embedding_size,\n",
        "                                        padding_idx = self.vocabulary_size -1\n",
        "                                        ).to(self.device)\n",
        "    self.encoder_lstm = nn.LSTM(input_size =  self.embedding_size,\n",
        "                                hidden_size = self.hidden_size,\n",
        "                                batch_first = True,\n",
        "                                bidirectional = True,\n",
        "                                num_layers = self.lstm_layers,\n",
        "                                dropout = self.encoder_dropout_p).to(self.device)\n",
        "    self.decoder_lstm = nn.LSTM(input_size= self.embedding_size,\n",
        "                                hidden_size = self.hidden_size,\n",
        "                                batch_first=True,                             \n",
        "                                num_layers = self.lstm_layers,\n",
        "                                dropout  = self.decoder_dropout_p).to(self.device)               \n",
        "    self.decoder_initialiser = nn.Linear(in_features= self.hidden_size*2,out_features= self.hidden_size).to(self.device)\n",
        "    self.distribution_nn = nn.Sequential(*[nn.Linear(in_features= self.hidden_size,out_features= self.vocabulary_size),\n",
        "                                        nn.LogSoftmax(dim=-1)\n",
        "                                        ]).to(self.device)\n",
        "    self.dropout = nn.Dropout(self.encoder_dropout_p).to(self.device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #################\n",
        "\n",
        "  # 'in_paragraphs' is a matrix (batch size, max in length) of character ids (Integer).\n",
        "  # 'out_paragraphs' is None at prediction time or a matrix (batch size, max out length) of character ids (Integer) at training time. Assume it does not include the final end-of-paragraph character.\n",
        "  \n",
        "  #this is for beam search,it build top k candidate at each time step (k is the beam width)\n",
        "  #it returns a list of k tuples : (candidate's real vocab_id,it's cumulative log probability,the beam position of previous candidate who generate this candidate)\n",
        "  def find_topk (self,cumulative_logp,beam_size):\n",
        "        flatten = cumulative_logp.view(1,-1)\n",
        "        topk_logp,topkid = torch.topk(flatten,k=beam_size)\n",
        "        topk_logp = topk_logp.squeeze() if beam_size >1 else topk_logp\n",
        "        topkid = topkid.squeeze() if beam_size >1 else topkid\n",
        "        new_candidate = []\n",
        "        if beam_size > 1:\n",
        "            for i in range(0,beam_size):\n",
        "              beampos = int((topkid[i])/cumulative_logp.size(1))\n",
        "              real_id = topkid[i]-beampos*cumulative_logp.size(1)\n",
        "              new_candidate.append((real_id,topk_logp[i],beampos))\n",
        "        else:\n",
        "            new_candidate.append((topkid,topk_logp,0))\n",
        "        return new_candidate\n",
        "\n",
        "      #Build a text_id list given the initial_candidate\n",
        "      #Since beampos is an indication of the parent node,it uses this information to retrieve text_ids from the initial_candidate\n",
        "  def build_from_candidate(self,candidates,initial_candidate,initial_index):\n",
        "        text_ids = [initial_candidate[0]] if initial_candidate[0] != self.EOP_id else []\n",
        "        beampos = initial_candidate[2]\n",
        "        candidates_slice  = candidates[:initial_index][::-1]\n",
        "        for c in candidates_slice:\n",
        "          if c[beampos][0] == self.EOP_id:\n",
        "              break\n",
        "          text_ids.append(c[beampos][0])\n",
        "          beampos = c[beampos][2]\n",
        "        return text_ids[::-1]\n",
        "\n",
        "      #Get all possible predictions from candidates\n",
        "      #it return a list of tuples (log_p,text_ids)\n",
        "  def get_all_predictions(self,candidates,EOP_id):\n",
        "        predictions = []\n",
        "        index = 0\n",
        "        #Add the best prediction from the last time step:\n",
        "        best_guess = max(candidates[-1],key = lambda x : x[1])\n",
        "        predictions.append((best_guess[1],self.build_from_candidate(candidates,best_guess,len(candidates)-1)))\n",
        "\n",
        "        #Add all other possible predictions with EOP mark\n",
        "        for c in candidates:\n",
        "            index +=  1\n",
        "            for tup in c:\n",
        "                if tup[0] == EOP_id:\n",
        "                    predictions.append((tup[1],self.build_from_candidate(candidates,tup,index)))\n",
        "\n",
        "        return predictions\n",
        "    \n",
        "  def add_first_char(self,out_paragraphs): # [Timothée Bernard] Que fait cette fonction ?\n",
        "        batch_size,max_length = out_paragraphs.size(0),out_paragraphs.size(1)\n",
        "        flatten = out_paragraphs.reshape(1,batch_size * max_length)[0] # [Timothée Bernard] ???????????????\n",
        "        first = torch.tensor([self.EOP_id],dtype=torch.long).to(self.device)\n",
        "        flatten = torch.cat((first,flatten[:-1])).to(self.device)\n",
        "        flatten = flatten.reshape(batch_size,max_length)\n",
        "        return flatten.to(self.device)\n",
        "  \n",
        "  def find_last_char(self,l):\n",
        "    i = len(l)-1\n",
        "    while l[i]==self.vocabulary_size or l[i]==0:\n",
        "      i-=1\n",
        "    return l[i]  \n",
        "\n",
        "\n",
        "  def forward(self, in_paragraphs, out_paragraphs=None, max_predicted_char=1000,beam_size = 1):\n",
        "    batch_size = in_paragraphs.size(0)\n",
        "\n",
        "    in_char_embeddings = self.dropout(self.char_embeddings(in_paragraphs)) # Shape: (batch_size, max length, embedding size)\n",
        "    #print(in_char_embeddings); print(in_char_embeddings.shape)\n",
        "    in_lengths = (in_paragraphs != self.char_embeddings.padding_idx).sum(axis=1) # Shape: (batch_size)\n",
        "    #print(in_lengths); print(in_lengths.shape)\n",
        "    in_char_embeddings = torch.nn.utils.rnn.pack_padded_sequence(input=in_char_embeddings, lengths=in_lengths.cpu(), batch_first=True, enforce_sorted=False) # Enables the LSTM to ignore padding elements.\n",
        "\n",
        "    # The input paragraphs are encoded; the final hidden and cell states of the network will be used to initialise the decoder after a little transformation.\n",
        "    _, (h_n, c_n) = self.encoder_lstm(in_char_embeddings) # 'h_n' and 'c_n' are both of shape (num_layers * 2, batch_size, lstm_unit)\n",
        "\n",
        "    # Concatenates the left-to-right and right-to-left final hidden states of the biLSTM.\n",
        "    h_n = h_n.view(self.encoder_lstm.num_layers, 2, batch_size, self.encoder_lstm.hidden_size) # The second dimension corresponds to left-to-right (0) and right-to-left (1).\n",
        "    #print(h_n); print(h_n.shape)\n",
        "    lr_h_n = h_n[:,0] # left-to-right; shape: (num_layers, batch_size, lstm_unit)\n",
        "    rl_h_n = h_n[:,1] # right-to-left; shape: (num_layers, batch_size, lstm_unit)\n",
        "    bi_h_n = torch.cat([lr_h_n, rl_h_n], axis=2) # Shape: (num_layers, batch_size, 2 * lstm_unit)\n",
        "    #print(bi_h_n); print(bi_h_n.shape)\n",
        "\n",
        "    # Concatenates the left-to-right and right-to-left final cell states of the biLSTM.\n",
        "    c_n = c_n.view(self.encoder_lstm.num_layers, 2, batch_size, self.encoder_lstm.hidden_size) # The second dimension corresponds to left-to-right (0) and right-to-left (1).\n",
        "    #print(c_n); print(c_n.shape)\n",
        "    lr_c_n = c_n[:,0] # left-to-right; shape: (num_layers, batch_size, lstm_unit)\n",
        "    rl_c_n = c_n[:,1] # right-to-left; shape: (num_layers, batch_size, lstm_unit)\n",
        "    bi_c_n = torch.cat([lr_c_n, rl_c_n], axis=2) # Shape: (num_layers, batch_size, 2 * lstm_unit)\n",
        "    #print(bi_c_n); print(bi_c_n.shape)\n",
        "\n",
        "    last_char = self.find_last_char(in_paragraphs[-1])\n",
        "    decoder_init_states = (self.decoder_initialiser(bi_h_n), self.decoder_initialiser(bi_c_n)) # These tensors are not only used to initialise the decoder but also (for the first tensor) to compute the probability distributions for the first character.\n",
        "    if(out_paragraphs is not None): # Training time: outputs the logits for each time step.\n",
        "      # Feed a packed sequence to the decoder (use 'torch.nn.utils.rnn.pack_padded_sequence' and 'torch.nn.utils.rnn.pad_packed_sequence').\n",
        "      #################\n",
        "      out_paragraphs = self.add_first_char(out_paragraphs)\n",
        "      out_char_embeddings = self.dropout((self.char_embeddings(out_paragraphs)))\n",
        "      out_lengths = (out_paragraphs != self.char_embeddings.padding_idx).sum(axis=1) # Shape: (batch_size)\n",
        "      out_char_embeddings = nn.utils.rnn.pack_padded_sequence(input=out_char_embeddings, lengths=out_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "      out,_ = self.decoder_lstm(out_char_embeddings,decoder_init_states)\n",
        "      out =  nn.utils.rnn.pad_packed_sequence(out, batch_first=True)[0]\n",
        "      prediction = self.distribution_nn(out)\n",
        "      #################\n",
        "\n",
        "    else: # Prediction time: generates a text up to 'max_predicted_char' character long for each paragraph in the batch.\n",
        "      # Decode 'decoder_init_states' into a matrix a character ids (on line per input paragraph in the batch) and then convert it to string of actual characters.\n",
        "      #################\n",
        "      #Vectorized beam search : batch_size = beam_size,set beam_size to 1 to switch to the greedy search\n",
        "      \n",
        "      dc_initial_token = self.char_embeddings(last_char).view(1,1,-1)\n",
        "      if decoder_init_states[0].size(1) > 1:\n",
        "        raise Exception ('Text generation do not support batch,please keep batch size = 1')\n",
        "      first_out,(h_n_dc,c_n_dc) = self.decoder_lstm(dc_initial_token,decoder_init_states) #h,c : nb_layers,beam_size,hidden_size\n",
        "      #expand hidden and cell to beam size\n",
        "      h_n_dc = h_n_dc.expand(-1,beam_size,-1).contiguous() if beam_size > 1 else h_n_dc\n",
        "      c_n_dc = c_n_dc.expand(-1,beam_size,-1).contiguous() if beam_size > 1 else c_n_dc\n",
        "      first_out = self.distribution_nn(first_out) #first_out  : batch_size = 1,seq_len = 1,vocab_size\n",
        "      topk_logp,topk_id = torch.topk(first_out,k = beam_size)\n",
        "      #candidates is a list of topk log probability item in each time step,each candidate is a tuple of (candidate's vocab_id,candidate'scumulative log probability,candidate's \"beam position\")\n",
        "      #beampos(beam position) is a interger that points to the posistion of previous candidate (a.k.a parent node) who generates the current candidate,so it always < beam width\n",
        "      #beampos is crucial for the navigation\n",
        "      #beam_batch is a list of topk ids at each time step,it used as the input for the next time step (beam_batch is not necessarily contains all last candidates)\n",
        "      if beam_size == 1:\n",
        "        candidates = [[(topk_id.squeeze(),topk_logp.squeeze(),0)]]\n",
        "        beam_batch = [topk_id.squeeze()]\n",
        "      else:\n",
        "        candidates  = [[(id,logp,beam_position) for id,logp,beam_position in zip(topk_id.squeeze(),topk_logp.squeeze(),range(beam_size))]]\n",
        "        beam_batch = [torch.tensor([c[0] for c in candidates[-1]]).to(self.device).view(beam_size,1)]\n",
        "      prev_logp = torch.tensor([c[1] for c in candidates[-1]]).view(-1,1).to(self.device)\n",
        "      for t in range(max_predicted_char):\n",
        "        out_char_embeddings = self.char_embeddings(beam_batch[-1])\n",
        "        if beam_size == 1:\n",
        "          out_char_embeddings = out_char_embeddings.view(1,1,-1)\n",
        "        out,(h_n_dc,c_n_dc) = self.decoder_lstm(out_char_embeddings,(h_n_dc,c_n_dc))\n",
        "        \n",
        "        logp = self.distribution_nn(out) #beam_size ,seq_len = 1,vocab_size\n",
        "        logp = logp.squeeze() #beam_size ,vocab_size\n",
        "        cumulative_logp = 1/(t+1) * (prev_logp + logp) if beam_size >1 else logp #normalized by current seq length,so it won't prefer generating long sentence\n",
        "        new_candidate = self.find_topk(cumulative_logp = cumulative_logp,beam_size = beam_size)\n",
        "        new_beam_batch = [c[0] for c in new_candidate if c[0] != self.EOP_id] #only candidate with no EOP mark will be added to the new_beam_batch\n",
        "        if new_beam_batch == []: #if new_beam_batch is empty,that means all possible candidates beam reach the EOP mark\n",
        "          break\n",
        "        new_beam_batch = torch.tensor(new_beam_batch).view(len(new_beam_batch),1).to(self.device)\n",
        "        prev_logp = torch.tensor([c[1] for c in new_candidate if c[0] != self.EOP_id]).view(-1,1).to(self.device)\n",
        "        beampos = torch.tensor([c[2] for c in new_candidate if c[0]!= self.EOP_id]).to(self.device) #the beam position of ecah candidate indicates its \"parent node\"\n",
        "        #modifier the hidden and cell state according the beampos of the current candidate\n",
        "        #this step is crucial for getting the right result \n",
        "        h_n_dc = torch.index_select(input = h_n_dc,dim = 1,index = beampos) if beam_size > 1 else h_n_dc\n",
        "        c_n_dc = torch.index_select(input = c_n_dc,dim = 1,index = beampos) if beam_size > 1 else c_n_dc\n",
        "        candidates.append(new_candidate)\n",
        "        beam_batch.append(new_beam_batch)\n",
        "      \n",
        "      #this use to retrieve the all possible generated text,including those candidate who reach EOP before the max generation length\n",
        "      #it will return a list of tuple  : (log_proba,text_ids)\n",
        "      prediction = self.get_all_predictions(candidates,self.EOP_id) \n",
        "      #get the best text_ids\n",
        "      best_guess = max(prediction,key=lambda x : x[0])\n",
        "      #convert text_ids to text\n",
        "      prediction = (best_guess[0],ids_to_texts([[int(i) for i in best_guess[1]]]))\n",
        "\n",
        "    return prediction\n",
        "        \n",
        "      \n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5TXsvizgogZ"
      },
      "source": [
        "model = Model(vocabulary_size=len(char_vocabulary), EOP_id=EOP_id, embedding_size=19, lstm_hidden_size=13, lstm_layers=7 ,encoder_dropout_p = 0.2,decoder_dropout_p = 0.1,device='cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA7PGEVvcsM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4bc3860-ed9b-4dd0-cd21-59ddaf5b1a47"
      },
      "source": [
        "\n",
        "in_paragraphs = torch.tensor([(list(range(5)) + ([batch_generator.padding_idx] * 0))]).to(model.device)\n",
        "print(in_paragraphs)\n",
        "out_paragraphs = in_paragraphs\n",
        "model(in_paragraphs, out_paragraphs)\n",
        "in_paragraphs = torch.tensor([(list(range(5)) + ([batch_generator.padding_idx] * 10)), (list(range(10)) + ([batch_generator.padding_idx] * 5))]).to(model.device)\n",
        "print(in_paragraphs)\n",
        "out_paragraphs = in_paragraphs\n",
        "model(in_paragraphs, out_paragraphs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1, 2, 3, 4]])\n",
            "tensor([[  0,   1,   2,   3,   4, 102, 102, 102, 102, 102, 102, 102, 102, 102,\n",
            "         102],\n",
            "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9, 102, 102, 102, 102,\n",
            "         102]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-4.8225, -4.4833, -4.9103,  ..., -4.8653, -4.4773, -4.4805],\n",
              "         [-4.8453, -4.5073, -4.9151,  ..., -4.8518, -4.5073, -4.4790],\n",
              "         [-4.8557, -4.5253, -4.9098,  ..., -4.8471, -4.5250, -4.4802],\n",
              "         ...,\n",
              "         [-4.8976, -4.4639, -4.9351,  ..., -4.8051, -4.5296, -4.4583],\n",
              "         [-4.8976, -4.4639, -4.9351,  ..., -4.8051, -4.5296, -4.4583],\n",
              "         [-4.8976, -4.4639, -4.9351,  ..., -4.8051, -4.5296, -4.4583]],\n",
              "\n",
              "        [[-4.8316, -4.4788, -4.9124,  ..., -4.8646, -4.4813, -4.4732],\n",
              "         [-4.8502, -4.5034, -4.9191,  ..., -4.8496, -4.5077, -4.4752],\n",
              "         [-4.8559, -4.5177, -4.9235,  ..., -4.8433, -4.5194, -4.4759],\n",
              "         ...,\n",
              "         [-4.8725, -4.5295, -4.9197,  ..., -4.8332, -4.5362, -4.4766],\n",
              "         [-4.8723, -4.5272, -4.9185,  ..., -4.8377, -4.5373, -4.4743],\n",
              "         [-4.8727, -4.5261, -4.9178,  ..., -4.8390, -4.5382, -4.4727]]],\n",
              "       grad_fn=<LogSoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRPGm_Duq_QY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "390bb816-0102-4254-df91-26dc40c32256"
      },
      "source": [
        "#The vectorized beam search do not support batch generation,the pro is ,since it dose not use loop,so the generation is much quicker\n",
        "#lease keep batch size = 1 for generating text\n",
        "batch = batch_generator.get_batch(2)\n",
        "for b  in batch:\n",
        "  t  =  model(b[0].view(1,-1).to(model.device), max_predicted_char=16)\n",
        "  print(t)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[-4.3680]], grad_fn=<TopkBackward0>), ['UUUÔÔÔÔÔÔÔÔÔÔÔÔÔx'])\n",
            "(tensor([[-4.3665]], grad_fn=<TopkBackward0>), ['UUUÔÔÔÔÔÔÔÔÔÔÔÔÔÔ'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80KIRPPyOCWQ"
      },
      "source": [
        "Training\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7nlm7UMBBkg"
      },
      "source": [
        "# 'logits' is of shape (batch size, max paragraph length, vocabulary size)\n",
        "# 'out_paragraphs' is of shape (batch size, max paragraph length)\n",
        "\n",
        "def loss_function_logits(logits, out_paragraphs,pad_id = PAD_id):\n",
        "  # Use 'torch.nn.functional.nll_loss'.\n",
        "  # Computes an average over all tokens of the batch, but do not take into account distribution logits corresponding to padding characters.\n",
        "  #################\n",
        "  if logits.size(1) < out_paragraphs.size(1):\n",
        "    out_paragraphs = out_paragraphs[:,:-1]\n",
        "  batch_size,max_length,vocabulary_size = logits.size()\n",
        "  logits = logits.reshape(batch_size*max_length,vocabulary_size)\n",
        "  golden = out_paragraphs.reshape(batch_size*max_length)\n",
        "  #pad_mask = golden != pad_id\n",
        "  return torch.nn.functional.nll_loss(logits,golden,ignore_index=pad_id)\n",
        "  #################\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cep0jryu22mh"
      },
      "source": [
        "def evaluation(prediction,gold):\n",
        "      p_max  = torch.argmax(prediction,dim  = 2)\n",
        "      nb_correct = 0\n",
        "      total = 0\n",
        "      current_batch_size,seq_len  = p_max.size()\n",
        "      flatten_pred  = p_max.view(current_batch_size*seq_len)\n",
        "      flatten_out =  gold[:,:-1].reshape(current_batch_size*seq_len) if  seq_len< gold.size(1) else  gold.reshape(batch_size*seq_len)\n",
        "      mask  = (flatten_out!=PAD_id)\n",
        "      nb_correct +=torch.sum((flatten_out==flatten_pred)*mask)\n",
        "      total   += torch.sum(mask)\n",
        "      accurracy = float(nb_correct)/float(total)\n",
        "      return accurracy\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DzP-FNGX7ho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e429bcd-bab8-4746-d188-1335c4456f7f"
      },
      "source": [
        "batch = batch_generator.get_batch(2)\n",
        "logits = model(batch[0].to(model.device), batch[1][:,:-1].to(model.device))\n",
        "loss_function_logits(logits, batch[1].to(model.device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.7157, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdJSBtNGCX-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30e7528-6a87-462f-a4d0-67e1a797e8f8"
      },
      "source": [
        "model = Model(vocabulary_size=len(char_vocabulary), EOP_id=EOP_id, embedding_size=256, lstm_hidden_size=512, lstm_layers=1 ,encoder_dropout_p = 0.2,decoder_dropout_p = 0.1,device='cuda')\n",
        "\n",
        "import time\n",
        "\n",
        "ratio =  lambda x : int(len(x)*0.9)\n",
        "\n",
        "batch_generators = []\n",
        "data = [paragraphs,paragraphs_2]\n",
        "for  d in data:\n",
        "    divide  = ratio(d)\n",
        "    batch_generators.append((BatchGenerator(paragraphs=d[:divide], char_vocabulary=char_vocabulary),\n",
        "                             BatchGenerator(paragraphs=d[divide:], char_vocabulary=char_vocabulary)))\n",
        "    \n",
        "for b in batch_generators:\n",
        "    print(f\"BOOK:{b[0].length()}\")\n",
        "            \n",
        "\n",
        "\n",
        "model.eval() \n",
        "\n",
        "# Training procedure\n",
        "learning_rate = 0.2\n",
        "momentum = 0.99 # 0.99\n",
        "l2_reg = 0.0001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=l2_reg) # Once the backward propagation has been done, call the 'step' method (with no argument) to update the parameters.\n",
        "#optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate,weight_decay=l2_reg)\n",
        "batch_size = 64\n",
        "subset = None # Use an integer to train on a smaller portion of the training set, otherwise use None.\n",
        "\n",
        "for batch_generator in batch_generators:\n",
        "    epoch_size = batch_generator[0].length() if(subset is None) else subset # In number of instances\n",
        "    nb_epoch = 20\n",
        "    epoch_id = 0 # Id of the current epoch\n",
        "    instances_processed = 0 # Number of instances trained on in the current epoch\n",
        "    epoch_loss = [] # Will contain the loss for each batch of the current epoch\n",
        "    time_0 = time.time()\n",
        "\n",
        "    while(epoch_id < nb_epoch):\n",
        "\n",
        "      model.train()\n",
        "      \n",
        "      model.zero_grad() \n",
        "\n",
        "      batch = batch_generator[0].get_batch(batch_size, subset=subset)\n",
        "      #print(ids_to_texts(batch[0])); print(ids_to_texts(batch[1]))\n",
        "      in_paragraphs = batch[0].to(model.device)\n",
        "      out_paragraphs = batch[1].to(model.device)\n",
        "      \n",
        "\n",
        "      ###################\n",
        "      prediction = model(in_paragraphs,out_paragraphs)\n",
        "      loss = loss_function_logits(prediction,out_paragraphs).to(model.device)\n",
        "      epoch_loss.append(loss.item())\n",
        "      loss.backward()\n",
        "      ###################\n",
        "      \n",
        "      optimizer.step() # Updates the parameters.\n",
        "              \n",
        "      instances_processed += batch_size\n",
        "      if(instances_processed > epoch_size):\n",
        "        print(f\"-- END OF EPOCH {epoch_id}.\")\n",
        "        print(f\"Average loss on training set: {sum(epoch_loss) / len(epoch_loss)}.\")\n",
        "\n",
        "        model.eval()\n",
        "        validationn_batch = batch_generator[1].all_batches(32, subset=subset)\n",
        "        val_loss = []\n",
        "        val_accuracy = []\n",
        "        for b in validationn_batch:\n",
        "          val_in = b[0].to(model.device)\n",
        "          val_out = b[1].to(model.device)\n",
        "          val_prediction = model(val_in,val_out)\n",
        "          val_loss.append( loss_function_logits(val_prediction,val_out).to(model.device).item())\n",
        "          val_accuracy.append(evaluation(val_prediction,val_out))\n",
        "        \n",
        "        \n",
        "        print(f\"Average accuracy on validation set:{sum(val_accuracy)/len(val_accuracy)}\")\n",
        "        print(f\"Average loss on validation set: {sum(val_loss) / len(val_loss)}.\")\n",
        "        duration = time.time() - time_0\n",
        "        print(f\"{duration} s elapsed (i.e. {duration / (epoch_id + 1)} s/epoch)\")\n",
        "\n",
        "\n",
        "        # Example of generation\n",
        "        batch = batch_generator[0].get_batch(1, subset=subset)\n",
        "        first = ids_to_texts(batch[0])\n",
        "        second = ids_to_texts(batch[1])\n",
        "        generated = model(batch[0].to(model.device), max_predicted_char=512)\n",
        "        score = sentence_bleu(second, generated[1])\n",
        "        print(f\"First: {first}\") # Input paragraph\n",
        "        print(f\"Second: {second}\")\n",
        "        print(f\"Generated: {generated[1]}\") # Generated output paragraph.\n",
        "        print(f\"BLEU score: {score}\")\n",
        "\n",
        "        epoch_id += 1\n",
        "        instances_processed -= epoch_size\n",
        "        epoch_loss = []\n",
        "        epoch_accuracy  = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOOK:2694\n",
            "BOOK:2589\n",
            "-- END OF EPOCH 0.\n",
            "Average loss on training set: 3.012395187865856.\n",
            "Average accuracy on validation set:0.29237121699484725\n",
            "Average loss on validation set: 2.460491895675659.\n",
            "63.578672647476196 s elapsed (i.e. 63.578672647476196 s/epoch)\n",
            "First: ['Il lut:\\n']\n",
            "Second: [\"-- «Malgré les préjugés qui recouvrent encore une partie de la face de l'Europe comme un réseau, la lumière cependant commence à pénétrer dans nos campagnes. C'est ainsi que, mardi, notre petite cité d'Yonville s'est vue le théâtre d'une expérience chirurgicale qui est en même temps un acte de haute philanthropie. M. Bovary, un de nos praticiens les plus distingués...»\\n\"]\n",
            "Generated: ['ZZZnt part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part part pa']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 1.\n",
            "Average loss on training set: 2.2898363726479665.\n",
            "Average accuracy on validation set:0.35861009597854787\n",
            "Average loss on validation set: 2.1646236896514894.\n",
            "125.59726095199585 s elapsed (i.e. 62.798630475997925 s/epoch)\n",
            "First: ['-- Que tu es charmante! dit-il en la saisissant dans ses bras.\\n']\n",
            "Second: [\"-- Vrai? fit-elle avec un rire de volupté. M'aimes-tu? Jure-le donc!\\n\"]\n",
            "Generated: ['-- Emme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme d']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 2.\n",
            "Average loss on training set: 2.0658981573014032.\n",
            "Average accuracy on validation set:0.40178302500672114\n",
            "Average loss on validation set: 1.987095022201538.\n",
            "189.44142413139343 s elapsed (i.e. 63.14714137713114 s/epoch)\n",
            "First: [\"-- Ah! c'est qu'il y en a deux, répliqua-t-il. La petite, la convenue, celle des hommes, celle qui varie sans cesse et qui braille si fort, s'agite en bas, terre à terre, comme ce rassemblement d'imbéciles que vous voyez. Mais l'autre, l'éternelle, elle est tout autour et au-dessus, comme le paysage qui nous environne et le ciel bleu qui nous éclaire.\\n\"]\n",
            "Second: [\"M. Lieuvain venait de s'essuyer la bouche avec son mouchoir de poche. Il reprit:\\n\"]\n",
            "Generated: ['-- Il se la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de la mardans de ']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 3.\n",
            "Average loss on training set: 1.9228323584511167.\n",
            "Average accuracy on validation set:0.43330973546156615\n",
            "Average loss on validation set: 1.8774540305137635.\n",
            "249.0309600830078 s elapsed (i.e. 62.25774002075195 s/epoch)\n",
            "First: [\"Ce fut alors qu'elle prit sa main, et ils restèrent quelque temps les doigts entrelacés, -- comme le premier jour, aux Comices! Par un geste d'orgueil, il se débattait sous l'attendrissement. Mais, s'affaissant contre sa poitrine, elle lui dit:\\n\"]\n",
            "Second: [\"-- Comment voulais-tu que je vécusse sans toi? On ne peut pas se déshabituer du bonheur! J'étais désespérée! j'ai cru mourir! Je te conterai tout cela, tu verras. Et toi... tu m'as fuie!...\\n\"]\n",
            "Generated: ['-- Elle se de son par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par les de se par l']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 4.\n",
            "Average loss on training set: 1.832762638727824.\n",
            "Average accuracy on validation set:0.45007695309234447\n",
            "Average loss on validation set: 1.806831431388855.\n",
            "311.8661425113678 s elapsed (i.e. 62.37322850227356 s/epoch)\n",
            "First: ['-- Mais calme-toi! disait madame Homais.\\n']\n",
            "Second: ['Et Athalie, le tirant par sa redingote\\n']\n",
            "Generated: [\"-- C'est par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les \"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 5.\n",
            "Average loss on training set: 1.761539291767847.\n",
            "Average accuracy on validation set:0.46643931154847956\n",
            "Average loss on validation set: 1.751742672920227.\n",
            "373.72744631767273 s elapsed (i.e. 62.28790771961212 s/epoch)\n",
            "First: ['-- Allons, bon voyage! leur dit-il, heureux mortels que vous êtes!\\n']\n",
            "Second: [\"Puis, s'adressant à Emma, qui portait une robe de soie bleue à quatre falbalas:\\n\"]\n",
            "Generated: [\"-- C'était de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comm\"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 6.\n",
            "Average loss on training set: 1.7087115191278004.\n",
            "Average accuracy on validation set:0.47817084481037825\n",
            "Average loss on validation set: 1.702494752407074.\n",
            "433.48710203170776 s elapsed (i.e. 61.92672886167254 s/epoch)\n",
            "First: [\"-- Je le crois très bête. Elle en est fatiguée sans doute. Il porte des ongles sales et une barbe de trois jours. Tandis qu'il trottine à ses malades, elle reste à ravauder des chaussettes. Et on s'ennuie! on voudrait habiter la ville, danser la polka tous les soirs! Pauvre petite femme! Ça bâille après l'amour, comme une carpe après l'eau sur une table de cuisine. Avec trois mots de galanterie, cela vous adorerait; j'en suis sûr! ce serait tendre! charmant!... Oui, mais comment s'en débarrasser ensuite?\\n\"]\n",
            "Second: [\"Alors les encombrements du plaisir, entrevus en perspective, le firent, par contraste, songer à sa maîtresse. C'était une comédienne de Rouen, qu'il entretenait; et, quand il se fut arrêté sur cette image, dont il avait, en souvenir même, des rassasiements:\\n\"]\n",
            "Generated: ['-- Ah! comme un par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le par le ']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 7.\n",
            "Average loss on training set: 1.6656943332581293.\n",
            "Average accuracy on validation set:0.48702536525502466\n",
            "Average loss on validation set: 1.666870129108429.\n",
            "494.0480580329895 s elapsed (i.e. 61.75600725412369 s/epoch)\n",
            "First: [\"Charles prit les violettes, et, rafraîchissant dessus ses yeux tout rouges de larmes, il les humait délicatement. Elle les retira vite de sa main, et alla les porter dans un verre d'eau.\\n\"]\n",
            "Second: [\"Le lendemain, madame Bovary mère arriva. Elle et son fils pleurèrent beaucoup. Emma, sous prétexte d'ordres à donner, disparut.\\n\"]\n",
            "Generated: [\"-- C'est de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte \"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 8.\n",
            "Average loss on training set: 1.6348883310953777.\n",
            "Average accuracy on validation set:0.4964153690951704\n",
            "Average loss on validation set: 1.6334491133689881.\n",
            "554.035418510437 s elapsed (i.e. 61.55949094560411 s/epoch)\n",
            "First: [\"Elle entra, comme autrefois, par la petite porte du parc, puis arriva à la cour d'honneur, que bordait un double rang de tilleuls touffus. Ils balançaient, en sifflant, leurs longues branches. Les chiens au chenil aboyèrent tous, et l'éclat de leurs voix retentissait sans qu'il parût personne.\\n\"]\n",
            "Second: [\"Elle monta le large escalier droit, à balustres de bois, qui conduisait au corridor pavé de dalles poudreuses où s'ouvraient plusieurs chambres à la file, comme dans les monastères ou les auberges. La sienne était au bout, tout au fond, à gauche. Quand elle vint à poser les doigts sur la serrure, ses forces subitement l'abandonnèrent. Elle avait peur qu'il ne fût pas là, le souhaitait presque, et c'était pourtant son seul espoir, la dernière chance de salut. Elle se recueillit une minute, et, retrempant son courage au sentiment de la nécessité présente, elle entra.\\n\"]\n",
            "Generated: ['-- Ah! contre les propte de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la co']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 9.\n",
            "Average loss on training set: 1.598639434292203.\n",
            "Average accuracy on validation set:0.5049684200144461\n",
            "Average loss on validation set: 1.6038880228996277.\n",
            "616.6100146770477 s elapsed (i.e. 61.66100146770477 s/epoch)\n",
            "First: [\"Ils arrivèrent à la nuit tombante, comme on commençait à allumer des lampions dans le parc, afin d'éclairer les voitures.\\n\"]\n",
            "Second: [\"Le château, de construction moderne, à l'Italienne, avec deux ailes avançant et trois perrons, se déployait au bas d'une immense pelouse où paissaient quelques vaches, entre des bouquets de grands arbres espacés, tandis que des bannettes d'arbustes, rhododendrons, seringas et boules-de-neige bombaient leurs touffes de verdure inégales sur la ligne courbe du chemin sablé. Une rivière passait sous un pont; à travers la brume, on distinguait des bâtiments à toit de chaume, éparpillés dans la prairie, que bordaient en pente douce deux coteaux couverts de bois, et par derrière, dans les massifs, se tenaient, sur deux lignes parallèles, les remises et les écuries, restes conservés de l'ancien château démoli.\\n\"]\n",
            "Generated: ['-- Eh! comme un autre de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les proches de la contre les']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 10.\n",
            "Average loss on training set: 1.5628395884536033.\n",
            "Average accuracy on validation set:0.5143244772948148\n",
            "Average loss on validation set: 1.5772834777832032.\n",
            "678.2091236114502 s elapsed (i.e. 61.6553748737682 s/epoch)\n",
            "First: [\"Pourtant il fallait que M. Lheureux s'en mêlât.\\n\"]\n",
            "Second: [\"-- Écoutez donc! il me semble que, jusqu'à présent, j'ai été assez bon pour vous.\\n\"]\n",
            "Generated: [\"-- C'est pas de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la perdue de la \"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 11.\n",
            "Average loss on training set: 1.541238234156654.\n",
            "Average accuracy on validation set:0.5183382059309489\n",
            "Average loss on validation set: 1.55585857629776.\n",
            "740.5967020988464 s elapsed (i.e. 61.716391841570534 s/epoch)\n",
            "First: ['Elle accoucha un dimanche, vers six heures, au soleil levant.\\n']\n",
            "Second: [\"-- C'est une fille! dit Charles.\\n\"]\n",
            "Generated: [\"-- C'est de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte \"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 12.\n",
            "Average loss on training set: 1.5146569808324177.\n",
            "Average accuracy on validation set:0.5258780130715135\n",
            "Average loss on validation set: 1.524293601512909.\n",
            "800.5711450576782 s elapsed (i.e. 61.58239577366756 s/epoch)\n",
            "First: [\"-- M. Léon; disait le pharmacien, avec qui j'en causais l'autre jour, s'étonne que vous ne choisissiez point Madeleine, qui est excessivement à la mode maintenant.\\n\"]\n",
            "Second: [\"Mais la mère Bovary se récria bien fort sur ce nom de pécheresse. M. Homais, quant à lui, avait en prédilection tous ceux qui rappelaient un grand homme, un fait illustre ou une conception généreuse, et c'est dans ce système-là qu'il avait baptisé ses quatre enfants. Ainsi, Napoléon représentait la gloire et Franklin la liberté; Irma, peut-être, était une concession au romantisme; mais Athalie, un hommage au plus immortel chef-d'oeuvre de la scène française. Car ses convictions philosophiques n'empêchaient pas ses admirations artistiques, le penseur chez lui n'étouffait point l'homme sensible; il savait établir des différences, faire la part de l'imagination et celle du fanatisme. De cette tragédie, par exemple, il blâmait les idées, mais il admirait le style; il maudissait la conception, mais il applaudissait à tous les détails, et s'exaspérait contre les personnages, en s'enthousiasmant de leurs discours. Lorsqu'il lisait les grands morceaux, il était transporté; mais, quand il songeait que les calotins en tiraient avantage pour leur boutique, il était désolé, et dans cette confusion de sentiments où il s'embarrassait, il aurait voulu tout à la fois pouvoir couronner Racine de ses deux mains et discuter avec lui pendant un bon quart d'heure.\\n\"]\n",
            "Generated: ['Et il se rentre la courait de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la charmes de la char']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 13.\n",
            "Average loss on training set: 1.4787942965825398.\n",
            "Average accuracy on validation set:0.5340433028688625\n",
            "Average loss on validation set: 1.4998236417770385.\n",
            "862.7015783786774 s elapsed (i.e. 61.621541312762666 s/epoch)\n",
            "First: [\"Et brusquement elle retira sa main, pour entrer dans la chapelle de la Vierge, où, s'agenouillant contre une chaise, elle se mit en prière.\\n\"]\n",
            "Second: [\"Le jeune homme fut irrité de cette fantaisie bigote; puis il éprouva pourtant un certain charme à la voir, au milieu du rendez-vous, ainsi perdue dans les oraisons comme une marquise andalouse; puis il ne tarda pas à s'ennuyer, car elle n'en finissait.\\n\"]\n",
            "Generated: [\"-- C'est pas le contre les partires de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la petite de la pet\"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 14.\n",
            "Average loss on training set: 1.4622118529819308.\n",
            "Average accuracy on validation set:0.5422341577239257\n",
            "Average loss on validation set: 1.4739275574684143.\n",
            "924.9272577762604 s elapsed (i.e. 61.661817185084026 s/epoch)\n",
            "First: [\"-- Ah! c'est là la question! Telle est effectivement la question: _That is the question!_ comme je lisais dernièrement dans le journal.\\n\"]\n",
            "Second: [\"Mais Emma, se réveillant, s'écria:\\n\"]\n",
            "Generated: ['-- Oh! continuait le pour le contre le porte de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la ']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 15.\n",
            "Average loss on training set: 1.432811907359532.\n",
            "Average accuracy on validation set:0.5494571598421778\n",
            "Average loss on validation set: 1.4528467774391174.\n",
            "986.3160555362701 s elapsed (i.e. 61.644753471016884 s/epoch)\n",
            "First: [\"On était aux premiers jours d'octobre. Il y avait du brouillard sur la campagne. Des vapeurs s'allongeaient à l'horizon, entre le contour des collines; et d'autres, se déchirant, montaient, se perdaient. Quelquefois, dans un écartement des nuées, sous un rayon de soleil, on apercevait au loin les toits d'Yonville, avec les jardins au bord de l'eau, les cours, les murs, et le clocher de l'église. Emma fermait à demi les paupières pour reconnaître sa maison, et jamais ce pauvre village où elle vivait ne lui avait semblé si petit. De la hauteur où ils étaient, toute la vallée paraissait un immense lac pâle, s'évaporant à l'air. Les massifs d'arbres, de place en place, saillissaient comme des rochers noirs; et les hautes lignes des peupliers, qui dépassaient la brume, figuraient des grèves que le vent remuait.\\n\"]\n",
            "Second: [\"À côté, sur la pelouse, entre les sapins, une lumière brune circulait dans l'atmosphère tiède. La terre, roussâtre comme de la poudre de tabac, amortissait le bruit des pas; et, du bout de leurs fers, en marchant, les chevaux poussaient devant eux des pommes de pin tombées.\\n\"]\n",
            "Generated: [\"-- C'est pas les parles de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la main de la\"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 16.\n",
            "Average loss on training set: 1.4053188562393188.\n",
            "Average accuracy on validation set:0.5523242050258851\n",
            "Average loss on validation set: 1.4333815574645996.\n",
            "1048.2802402973175 s elapsed (i.e. 61.66354354690103 s/epoch)\n",
            "First: [\"Rodolphe avait un grand manteau; il l'en enveloppait tout entière, et, passant le bras autour de sa taille, il l'entraînait sans parler jusqu'au fond du jardin.\\n\"]\n",
            "Second: [\"C'était sous la tonnelle, sur ce même banc de bâtons pourris où autrefois Léon la regardait si amoureusement, durant les soirs d'été. Elle ne pensait guère à lui maintenant.\\n\"]\n",
            "Generated: [\"-- Ah! que l'on avait pas de la porte de la porte de son pas de la porte de la porte de son pas de la porte de la porte de son pas de la porte de la porte de son pas de la porte de la porte de son pas de la porte de la porte de son pas de la porte de la porte de son pas de la porte de la porte de son pas de la porte de la porte de son pas de la porte de la porte de son pas de la porte de la porte de son pas de la porte de la porte de son pas de la porte de la porte de son pas de la porte de la porte de son p\"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 17.\n",
            "Average loss on training set: 1.3877355541501726.\n",
            "Average accuracy on validation set:0.5605033176468002\n",
            "Average loss on validation set: 1.4177544355392455.\n",
            "1109.9936544895172 s elapsed (i.e. 61.66631413830651 s/epoch)\n",
            "First: [\"-- Vous n'avez pas changé, vous êtes toujours charmante!\\n\"]\n",
            "Second: ['-- Oh! reprit-elle amèrement, ce sont de tristes charmes, mon ami, puisque vous les avez dédaignés.\\n']\n",
            "Generated: ['-- Ah! tu mais le considérait de la contre les considérations de la contre les considérations de la contre les considérations de la contre les considérations de la contre les considérations de la contre les considérations de la contre les considérations de la contre les considérations de la contre les considérations de la contre les considérations de la contre les considérations de la contre les considérations de la contre les considérations de la contre les considérations de la contre les considérations de ']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 18.\n",
            "Average loss on training set: 1.3675925532976787.\n",
            "Average accuracy on validation set:0.5649151487669954\n",
            "Average loss on validation set: 1.3944770097732544.\n",
            "1174.932047367096 s elapsed (i.e. 61.83852880879452 s/epoch)\n",
            "First: [\"Comme elle se plaignait de Tostes continuellement, Charles imagina que la cause de sa maladie était sans doute dans quelque influence locale, et, s'arrêtant à cette idée, il songea sérieusement à aller s'établir ailleurs.\\n\"]\n",
            "Second: [\"Dès lors, elle but du vinaigre pour se faire maigrir, contracta une petite toux sèche et perdit complètement l'appétit.\\n\"]\n",
            "Generated: ['-- Ah! tout en se rentrait de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la ']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 19.\n",
            "Average loss on training set: 1.350610179560525.\n",
            "Average accuracy on validation set:0.5716582679067078\n",
            "Average loss on validation set: 1.3800658464431763.\n",
            "1236.955097436905 s elapsed (i.e. 61.84775487184525 s/epoch)\n",
            "First: [\"Et tour à tour examinant la feuille de papier, puis la vieille femme, il répétait d'un ton paternel:\\n\"]\n",
            "Second: ['-- Approchez, approchez!\\n']\n",
            "Generated: ['-- Et elle se retira de sa contre les partiers de la convenaient de la contre les partiers de la convenaient de la contre les partiers de la convenaient de la contre les partiers de la convenaient de la contre les partiers de la convenaient de la contre les partiers de la convenaient de la contre les partiers de la convenaient de la contre les partiers de la convenaient de la contre les partiers de la convenaient de la contre les partiers de la convenaient de la contre les partiers de la convenaient de la co']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 0.\n",
            "Average loss on training set: 1.5461689640836018.\n",
            "Average accuracy on validation set:0.567796352016477\n",
            "Average loss on validation set: 1.4134081337187026.\n",
            "77.87640953063965 s elapsed (i.e. 77.87640953063965 s/epoch)\n",
            "First: [\"--Quand je disais que cela s'arrangerait! dit l'évêque.\\n\"]\n",
            "Second: ['Puis il ajouta en souriant:\\n']\n",
            "Generated: [\"---Je ne se travais de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre de l'autre chambre\"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 1.\n",
            "Average loss on training set: 1.4713222712278367.\n",
            "Average accuracy on validation set:0.5795919352087341\n",
            "Average loss on validation set: 1.3622772031360202.\n",
            "152.8916437625885 s elapsed (i.e. 76.44582188129425 s/epoch)\n",
            "First: ['--Que faites-vous donc là?\\n']\n",
            "Second: ['M. Madeleine était à cette place depuis une heure. Il attendait que Fantine se réveillât. Il lui prit la main, lui tâta le pouls, et répondit:\\n']\n",
            "Generated: ['--Ce que le conseignait de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la première de la pre']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 2.\n",
            "Average loss on training set: 1.436351758677785.\n",
            "Average accuracy on validation set:0.5908015837140649\n",
            "Average loss on validation set: 1.3226393063863118.\n",
            "228.55773973464966 s elapsed (i.e. 76.18591324488322 s/epoch)\n",
            "First: ['Puis elle éclata de rire et lui cracha au visage.\\n']\n",
            "Second: [\"M. Madeleine s'essuya le visage, et dit:\\n\"]\n",
            "Generated: ['La chambre et la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de la contentionnel de']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 3.\n",
            "Average loss on training set: 1.3964581102132798.\n",
            "Average accuracy on validation set:0.5966086144572683\n",
            "Average loss on validation set: 1.2962464094161987.\n",
            "302.72475028038025 s elapsed (i.e. 75.68118757009506 s/epoch)\n",
            "First: [\"--Combien m'en donneriez-vous? dit-elle.\\n\"]\n",
            "Second: ['--Dix francs.\\n']\n",
            "Generated: ['--Monsieur le paysant de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 4.\n",
            "Average loss on training set: 1.3545308374777072.\n",
            "Average accuracy on validation set:0.5999358413535427\n",
            "Average loss on validation set: 1.2817520433002048.\n",
            "375.3078489303589 s elapsed (i.e. 75.06156978607177 s/epoch)\n",
            "First: ['--Pourquoi?\\n']\n",
            "Second: ['--Les chevaux prennent toute la place.\\n']\n",
            "Generated: [\"--Monsieur le pays de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre\"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 5.\n",
            "Average loss on training set: 1.335189661383629.\n",
            "Average accuracy on validation set:0.6087665625133264\n",
            "Average loss on validation set: 1.2557220326529608.\n",
            "453.55942034721375 s elapsed (i.e. 75.59323672453563 s/epoch)\n",
            "First: ['--Monsieur le maire a-t-il réfléchi que nous sommes en hiver?...\\n']\n",
            "Second: ['M. Madeleine ne répondit pas. Le Flamand reprit:\\n']\n",
            "Generated: ['--Ce qui ne sait pas de la porte de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de l']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 6.\n",
            "Average loss on training set: 1.302954845312165.\n",
            "Average accuracy on validation set:0.6113184183950691\n",
            "Average loss on validation set: 1.2436743842230902.\n",
            "523.9266965389252 s elapsed (i.e. 74.84667093413216 s/epoch)\n",
            "First: ['Chapitre IX\\n']\n",
            "Second: ['Nouveaux griefs\\n']\n",
            "Generated: ['--Monsieur le conscience de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la po']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 7.\n",
            "Average loss on training set: 1.3047791421413422.\n",
            "Average accuracy on validation set:0.6133444425824792\n",
            "Average loss on validation set: 1.2340128421783447.\n",
            "596.2997436523438 s elapsed (i.e. 74.53746795654297 s/epoch)\n",
            "First: ['Chapitre I\\n']\n",
            "Second: [\"Le soir d'un jour de marche\\n\"]\n",
            "Generated: [\"Ce sont de l'autre chose de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolution de la révolu\"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 8.\n",
            "Average loss on training set: 1.2825687513118837.\n",
            "Average accuracy on validation set:0.6158034930211727\n",
            "Average loss on validation set: 1.2235861751768324.\n",
            "672.4499266147614 s elapsed (i.e. 74.71665851275127 s/epoch)\n",
            "First: [\"La belle hôtellerie s'était fermée pour lui; il cherchait quelque cabaret bien humble, quelque bouge bien pauvre.\\n\"]\n",
            "Second: [\"Précisément une lumière s'allumait au bout de la rue; une branche de pin, pendue à une potence en fer, se dessinait sur le ciel blanc du crépuscule. Il y alla.\\n\"]\n",
            "Generated: ['--Mais je ne suis pas monsieur le maire, mais il se rendre pas en place de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la por']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 9.\n",
            "Average loss on training set: 1.2826450735330581.\n",
            "Average accuracy on validation set:0.6168559548786514\n",
            "Average loss on validation set: 1.2156682676739163.\n",
            "748.698477268219 s elapsed (i.e. 74.8698477268219 s/epoch)\n",
            "First: ['Il dit:\\n']\n",
            "Second: ['--Non, madame.\\n']\n",
            "Generated: [\"--C'est une fois que le présence de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte \"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 10.\n",
            "Average loss on training set: 1.2510722368955611.\n",
            "Average accuracy on validation set:0.6222141264177996\n",
            "Average loss on validation set: 1.2053344117270575.\n",
            "818.0546853542328 s elapsed (i.e. 74.36860775947571 s/epoch)\n",
            "First: [\"C'était Fantine. Difficile à reconnaître. Pourtant, à l'examiner attentivement, elle avait toujours sa beauté. Un pli triste, qui ressemblait à un commencement d'ironie, ridait sa joue droite. Quant à sa toilette, cette aérienne toilette de mousseline et de rubans qui semblait faite avec de la gaîté, de la folie et de la musique, pleine de grelots et parfumée de lilas, elle s'était évanouie comme ces beaux givres éclatants qu'on prend pour des diamants au soleil; ils fondent et laissent la branche toute noire.\\n\"]\n",
            "Second: [\"Dix mois s'étaient écoulés depuis «la bonne farce».\\n\"]\n",
            "Generated: ['Le président de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la po']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 11.\n",
            "Average loss on training set: 1.2538601770633604.\n",
            "Average accuracy on validation set:0.6218786013752868\n",
            "Average loss on validation set: 1.2053189145194159.\n",
            "899.8675932884216 s elapsed (i.e. 74.98896610736847 s/epoch)\n",
            "First: ['--Monsieur a son passeport?\\n']\n",
            "Second: ['--Oui.\\n']\n",
            "Generated: [\"--C'est un peu de ce qui est un peu de plus de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte d\"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 12.\n",
            "Average loss on training set: 1.2513113468885422.\n",
            "Average accuracy on validation set:0.6216873414019916\n",
            "Average loss on validation set: 1.1994566652509902.\n",
            "979.4764814376831 s elapsed (i.e. 75.34434472597562 s/epoch)\n",
            "First: [\"Maintenant, si l'on admet un moment avec nous que dans tout homme il y a une des espèces animales de la création, il nous sera facile de dire ce que c'était que l'officier de paix Javert.\\n\"]\n",
            "Second: ['Les paysans asturiens sont convaincus que dans toute portée de louve il y a un chien, lequel est tué par la mère, sans quoi en grandissant il dévorerait les autres petits.\\n']\n",
            "Generated: [\"--Monsieur le maire, c'est un coup de ce que cela ne se passait pas de la porte de la porte de la courrise, et l'ancien en ce que cela ne se passait pas de la porte de la porte de la courrise, et l'ancien en ce que cela ne se passait pas de la porte de la porte de la courrise, et l'ancien en ce que cela ne se passait pas de la porte de la porte de la courrise, et l'ancien en ce que cela ne se passait pas de la porte de la porte de la courrise, et l'ancien en ce que cela ne se passait pas de la porte de la po\"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 13.\n",
            "Average loss on training set: 1.2179998537389243.\n",
            "Average accuracy on validation set:0.6221933608473117\n",
            "Average loss on validation set: 1.1980983813603718.\n",
            "1055.3165719509125 s elapsed (i.e. 75.37975513935089 s/epoch)\n",
            "First: ['--Maître Scaufflaire, demanda-t-il, avez-vous un bon cheval?\\n']\n",
            "Second: [\"--Monsieur le maire, dit le Flamand, tous mes chevaux sont bons. Qu'entendez-vous par un bon cheval?\\n\"]\n",
            "Generated: [\"--Monsieur le maire est là de ce qu'il faisait en cette chambre de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la\"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 14.\n",
            "Average loss on training set: 1.218496498465538.\n",
            "Average accuracy on validation set:0.6256974022893247\n",
            "Average loss on validation set: 1.1862164338429768.\n",
            "1133.2067244052887 s elapsed (i.e. 75.54711496035257 s/epoch)\n",
            "First: [\"Çà et là, il s'arrêtait, parlait aux petits garçons et aux petites filles et souriait aux mères. Il visitait les pauvres tant qu'il avait de l'argent; quand il n'en avait plus, il visitait les riches.\\n\"]\n",
            "Second: [\"Comme il faisait durer ses soutanes beaucoup de temps, et qu'il ne voulait pas qu'on s'en aperçût, il ne sortait jamais dans la ville autrement qu'avec sa douillette violette. Cela le gênait un peu en été.\\n\"]\n",
            "Generated: ['--Monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 15.\n",
            "Average loss on training set: 1.2005999844248703.\n",
            "Average accuracy on validation set:0.6291401307600564\n",
            "Average loss on validation set: 1.1825962861378987.\n",
            "1209.4948287010193 s elapsed (i.e. 75.5934267938137 s/epoch)\n",
            "First: [\"--Les enfants, s'écria la mère Thénardier, comme ça se connaît tout de suite! les voilà qu'on jurerait trois soeurs!\\n\"]\n",
            "Second: [\"Ce mot fut l'étincelle qu'attendait probablement l'autre mère. Elle saisit la main de la Thénardier, la regarda fixement, et lui dit:\\n\"]\n",
            "Generated: ['--Monsieur le maire ne se reculevait pas que la maison sur la chambre et le conventionnel de la chambre et le pays de la porte de la chambre et le pays de la porte de la chambre et le pays de la porte de la chambre et le pays de la porte de la chambre et le pays de la porte de la chambre et le pays de la porte de la chambre et le pays de la porte de la chambre et le pays de la porte de la chambre et le pays de la porte de la chambre et le pays de la porte de la chambre et le pays de la porte de la chambre et']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 16.\n",
            "Average loss on training set: 1.1967062473297119.\n",
            "Average accuracy on validation set:0.6310720435926535\n",
            "Average loss on validation set: 1.1730470524893866.\n",
            "1279.2689816951752 s elapsed (i.e. 75.25111657030442 s/epoch)\n",
            "First: [\"Il lui semblait qu'on pouvait le voir.\\n\"]\n",
            "Second: ['Qui, on?\\n']\n",
            "Generated: ['--Monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire, monsieur le maire']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 17.\n",
            "Average loss on training set: 1.1944629564517881.\n",
            "Average accuracy on validation set:0.6319962823521997\n",
            "Average loss on validation set: 1.1729663875367906.\n",
            "1358.594796180725 s elapsed (i.e. 75.47748867670695 s/epoch)\n",
            "First: [\"C'était le petit garçon de la vieille.\\n\"]\n",
            "Second: [\"--Monsieur, dit-il, c'est moi qui vous ai procuré la carriole.\\n\"]\n",
            "Generated: [\"--Monsieur le maire de monseigneur Bienvenu était au moins de la première fois qu'il avait été comme un peu de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de plus de pl\"]\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 18.\n",
            "Average loss on training set: 1.190365406870842.\n",
            "Average accuracy on validation set:0.6302819340781183\n",
            "Average loss on validation set: 1.1713791688283284.\n",
            "1440.2176041603088 s elapsed (i.e. 75.8009265347531 s/epoch)\n",
            "First: [\"M. Myriel devait subir le sort de tout nouveau venu dans une petite ville où il y a beaucoup de bouches qui parlent et fort peu de têtes qui pensent. Il devait le subir, quoiqu'il fût évêque et parce qu'il était évêque. Mais, après tout, les propos auxquels on mêlait son nom n'étaient peut-être que des propos; du bruit, des mots, des paroles; moins que des paroles, des _palabres_, comme dit l'énergique langue du midi.\\n\"]\n",
            "Second: [\"Quoi qu'il en fût, après neuf ans d'épiscopat et de résidence à Digne, tous ces racontages, sujets de conversation qui occupent dans le premier moment les petites villes et les petites gens, étaient tombés dans un oubli profond. Personne n'eût osé en parler, personne n'eût même osé s'en souvenir.\\n\"]\n",
            "Generated: ['--Je vous donne un considérer le conventionnel de la rentre de la rente de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la por']\n",
            "BLEU score: 0\n",
            "-- END OF EPOCH 19.\n",
            "Average loss on training set: 1.170850230426323.\n",
            "Average accuracy on validation set:0.6328182605552303\n",
            "Average loss on validation set: 1.1716512044270833.\n",
            "1515.953320980072 s elapsed (i.e. 75.7976660490036 s/epoch)\n",
            "First: [\"M. Madeleine se hâta d'écrire aux Thénardier. Fantine leur devait cent vingt francs. Il leur envoya trois cents francs en leur disant de se payer sur cette somme, et d'amener tout de suite l'enfant à Montreuil-sur-mer où sa mère malade la réclamait.\\n\"]\n",
            "Second: ['Ceci éblouit le Thénardier.\\n']\n",
            "Generated: [\"--Monsieur le maire, dit l'évêque, dit l'évêque, l'autre au moins de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main de la porte de la main\"]\n",
            "BLEU score: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxs4UZRAHxx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a76d4f1-e374-4632-f1cb-34b0780b7baa"
      },
      "source": [
        "#The vectorized beam search do not support batch generation,the pro is ,since it dose not use loop,so the generation is much quicker\n",
        "test_generator = BatchGenerator(paragraphs = paragraphs, char_vocabulary = char_vocabulary)\n",
        "text = [\"Charles Bovary sortit une bonne bouteille de vin et alla chercher des verres pour ses invités.\"]\n",
        "print(text)\n",
        "for i in range(10):\n",
        "  batch = test_generator.turn_into_batch(text)\n",
        "  text = model(batch.to(model.device),beam_size  = 30, max_predicted_char=1024)[1]\n",
        "  print(text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Charles Bovary sortit une bonne bouteille de vin et alla chercher des verres pour ses invités.']\n",
            "[\"é le mourant de l'ét l'autre\"]\n",
            "[\"e l'évêque entra dans la chambre de loite de l'étranger avant d'autour de l'évêque\"]\n",
            "['utre']\n",
            "[\"e l'évêque era dans l'ambre de l'étroite de l'évêque en'étévêque entra des pro pays, et leétatroite de l'étroite de l'évêque entra dans l'ouverture de l'évêque entra dans l'ombre de l'étroite de l'étr son coeur de l'auberge.e l'étroite de la \"]\n",
            "[\"de l'évêque enroite de l'étroite de l'évêque entra porte de l'évêque enra dans la chambre de l'étroite de l'étroite de l'évêque et lui demanda la de l'ét l'ae l'étroite de l'autre\"]\n",
            "[\"e l'évêque entra dans l'ombre de l'étroite de l'étranger au moment oe l'étrotroite de l'évêque, \"]\n",
            "[\"ns l'ouverture de l'évêque et l de sa porte de l'étranger avanévêque entre ltroite dbre de l'évêque ene l'auberge de l'évêque entra dans l'ombre de l'évêque et \"]\n",
            "[\"e l'évêque evée,'ae main slite de l'évêque entra dans le place de l'étroite de l'étroite de l'évêque enombre de l'étroit droite de l'étroite de l'étroite de l'évêque enminée de l'tra dans la chambre de ltra dans la cheminée de l'évêque enmbre de l'étroite de l'évêque de l'évêque êque et lui dit: évêque entra dans l'ombre de l'étroit de l'étranger avant d'autorité de l'rtroit de l'autorité de l'étroite de l'évêque entra dans la chambre de l'étroite de l'évêque\"]\n",
            "['utre']\n",
            "[\"e l'évêque era dans l'ambre de l'étroite de l'évêque en'étévêque entra des pro pays, et leétatroite de l'étroite de l'évêque entra dans l'ouverture de l'évêque entra dans l'ombre de l'étroite de l'étr son coeur de l'auberge.e l'étroite de la \"]\n"
          ]
        }
      ]
    }
  ]
}